# ğŸ  Moscow Housing Price Prediction â€” Sberbank Dataset (Course Project)

## ğŸ“Œ Summary

This project was completed as part of a university machine learning course. We used the **Sberbank Russian Housing Market** dataset to predict apartment sale prices in Moscow between 2011 and 2016.  

Our model achieved a **score equivalent to the actual 5th place solution** on the private leaderboard.

---

## ğŸ§  We used this opportunity to:

- Clean and structure real-world tabular data
- Perform exploratory data analysis to understand pricing patterns
- Engineer features based on domain logic and spatial information
- Apply and compare machine learning models (XGBoost, Random Forest)
- Tune hyperparameters and reduce feature redundancy with PCA
- Build interpretable and reproducible analysis in notebooks

---

## ğŸ› ï¸ Tools & Libraries

- Python  
- Jupyter Notebooks  
- Pandas, NumPy  
- Scikit-learn, XGBoost  
- Matplotlib, Seaborn  
- PCA, Bayesian Optimization

---

## ğŸ“ Repository Structure

/Data                  â†’ Raw and cleaned CSV files
/Word Documents        â†’ Report and documentation
/project               â†’ Notebooks and model code
/text_files            â†’ Supporting notes
cleaned_train.csv      â†’ Cleaned training data
cleaned_test.csv       â†’ Cleaned test data
macro.csv              â†’ Macroeconomic indicators
train.csv / test.csv   â†’ Original Sberbank data
README.md              â†’ This file

---

## ğŸ” Approach

### 1. Data Cleaning
- Dropped columns with excessive missing values or low correlation with price  
- Filled missing values using informed estimates (e.g. kitchen/living area ratios per region)  
- Fixed inconsistencies (e.g. unrealistic floor counts, invalid timestamps)

### 2. Exploratory Data Analysis (EDA)
- Explored price differences by property type (owner-occupied vs. investment)  
- Analyzed macro trends (e.g. 2015â€“2016 economic decline)  
- Visualized feature distributions and outliers

### 3. Feature Engineering
- Created new variables such as size ratios and temporal indicators  
- Encoded location data using smoothed average price per sub-area:

  \[
  \text{Encoded Price} = \left(\frac{n}{n + m}\right) \cdot \text{Local Avg} + \left(1 - \frac{n}{n + m}\right) \cdot \text{Global Avg}
  \]

### 4. Modeling
- Built separate models for each product type  
- Compared XGBoost and Random Forest performance  
- Tuned models using Bayesian optimization  
- Applied PCA to reduce multicollinearity and preserve 95% of variance

### 5. Ensembling
- Trained multiple models per segment  
- Aggregated predictions using the median  
- Applied a scaling factor to align predictions with real-world pricing levels

---

## âš ï¸ Challenges

- Incomplete and inconsistent data across years  
- Capturing geographic pricing trends without overfitting  
- Modeling temporal shifts in economic conditions  
- Balancing accuracy with interpretability

---
## ğŸ™Œ Credits

- Special thanks to my teammates **Lidor Erez** and **Dvir Rehavi**  
- Together, we chose to treat this academic project as an opportunity to go beyond course expectations.  
- The greatest takeaway from this experience was the value of **collaboration** â€” working as a focused, aligned team made this project far more impactful than anything we could have done alone.

---


## ğŸš€ How to Run

```bash
git clone https://github.com/danmiz/mosc_housing_price_pred.git
cd mosc_housing_price_pred
pip install -r requirements.txt
jupyter notebook project/Sberbank_Pipeline.ipynb
